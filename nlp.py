# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15iTMEtaEEIWx0A6pbGJrbaLW37NAgtdX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('train.txt',sep=';',header=None,names=['text','emotion'])

df.head()

df.isnull().sum()

unique_emotions = df['emotion'].unique()
emotion_numbers = {}
i=0
for emo in unique_emotions:
  emotion_numbers[emo] = i
  i += 1
# The mapping should happen after the emotion_numbers dictionary is fully populated.
df['emotion'] = df['emotion'].map(emotion_numbers)

df

df['text'] = df['text'].apply(lambda x : x.lower())

import string
def remove_punctuation(text):
  return text.translate(str.maketrans('','',string.punctuation))

df['text'] = df['text'].apply(remove_punctuation)

def remove_numbers(txt):
  new=""
  for i in txt:
    if not i.isdigit():
      new = new + i
  return new

  df['text'] = df['text'].apply(remove_numbers)

def remove_emojis(txt):
  new=""
  for i in txt:
    if i.isascii():
      new = new + i
  return new

  df['text'] = df['text'].apply(remove_emojis)

import nltk

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
stop_words

len(stop_words)

df.loc[1]['text']

def remove(txt):
  words = word_tokenize(txt)
  # filtered_words = [word for word in words if word.lower() not in stop_words]
  # return ' '.join(filtered_words)
  cleaned = []
  for i in words:
    if i not in stop_words:
      cleaned.append(i)
  return ' '.join(cleaned)

import nltk
nltk.download('punkt_tab')

df['text'] = df['text'].apply(remove)

df.loc[1]['text']

